{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import StringType,BooleanType,DateType,IntegerType,DoubleType\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "#create spark Session\n",
    "spark = SparkSession.builder.appName(\"PF\").config(\"spark.sql.caseSensitive\", \"True\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: integer (nullable = true)\n",
      " |-- Account: string (nullable = true)\n",
      " |-- Item: string (nullable = true)\n",
      " |-- Amount: double (nullable = true)\n",
      " |-- Date: date (nullable = true)\n",
      " |-- Transaction Type: string (nullable = true)\n",
      " |-- Categories: string (nullable = true)\n",
      " |-- Categories 2: string (nullable = true)\n",
      " |-- Real Amount: double (nullable = true)\n",
      " |-- Note: string (nullable = true)\n",
      " |-- Subscriptions: boolean (nullable = true)\n",
      " |-- Return: boolean (nullable = true)\n",
      " |-- Limit: double (nullable = true)\n",
      " |-- Account Type: string (nullable = true)\n",
      " |-- Owner: string (nullable = true)\n",
      "\n",
      "+--------------------+--------------------+-------+----------+----------------+-------------+-------------+-----------+------------------+-------------+------+------------+-----+\n",
      "|             Account|                Item| Amount|      Date|Transaction Type|   Categories| Categories 2|Real Amount|              Note|Subscriptions|Return|Account Type|Owner|\n",
      "+--------------------+--------------------+-------+----------+----------------+-------------+-------------+-----------+------------------+-------------+------+------------+-----+\n",
      "|Capital One Quick...|             Comcast|  39.99|2023-06-03|         Expense|      Housing|    Utilities|     -39.99|                  |        false| false|      Credit|   PA|\n",
      "| Discover It PA 1762|         Chick-fil-a|   2.77|2023-06-02|         Expense|       Dining|       Drinks|      -2.77|                  |        false| false|      Credit| Ritu|\n",
      "| Discover It PA 1762|         Chick-fil-a|   16.3|2023-06-02|         Expense|       Dining|         Food|      -16.3|                  |        false| false|      Credit| Ritu|\n",
      "| Chime Checking 4288|                 Ati|   10.0|2023-06-02|         Expense|Entertainment|       Movies|      -10.0|                  |        false| false|       Debit| Ritu|\n",
      "|Checking BoA Ritu...|              Klarna|  12.32|2023-06-02|         Expense|     Shopping|      Clothes|     -12.32|                  |        false| false|       Debit| Ritu|\n",
      "|Checking BoA PA 7462|Emd Millipore Co ...|  250.0|2023-06-02|          Income|       Income|          Job|      250.0|                  |        false| false|       Debit|   PA|\n",
      "|Adv Plus Banking ...|Emd Millipore Co ...|2389.91|2023-06-02|          Income|       Income|          Job|    2389.91|                  |        false| false|       Debit|Joint|\n",
      "|Adv Plus Banking ...|          Craigslist|    5.0|2023-06-02|         Expense|        Other|        Event|       -5.0|    Craiglist post|        false| false|       Debit|Joint|\n",
      "|Adv Plus Banking ...|Checkcard 0601 Ku...|  150.0|2023-06-02|         Expense|        Legal|  Immigration|     -150.0|                  |        false| false|       Debit|Joint|\n",
      "|Empower Customer ...|Interest Paid By ...|  33.07|2023-06-01|          Income|       Income|     Interest|      33.07|                  |        false| false|       Debit|   PA|\n",
      "| Chime Checking 4288|Emd Millipore Co,...|  250.0|2023-06-01|          Income|       Income|          Job|      250.0|                  |        false| false|       Debit| Ritu|\n",
      "|BoA Customized Ca...|          Aliexpress|  57.59|2023-06-01|         Expense|        Other|        Event|     -57.59|Wedding Decoration|        false| false|      Credit|   PA|\n",
      "|Adv Plus Banking ...|               Zelle|  100.0|2023-06-01|         Expense|     Business|   Tung Duong|     -100.0|                  |        false| false|       Debit|Joint|\n",
      "|Adv Plus Banking ...|Bill Pay Check 50...| 1600.0|2023-06-01|         Expense|      Housing|         Rent|    -1600.0|                  |        false| false|       Debit|Joint|\n",
      "| Robinhoodinvestment|Penn $26 Call 6/2...|   10.0|2023-05-31|         Income3|   Investment|Transactional|       10.0|                  |        false| false|  Investment|Joint|\n",
      "| Robinhoodinvestment|Penn $26 Call 6/2...|    0.0|2023-05-31|         Income3|   Investment|Transactional|        0.0|                  |        false| false|  Investment|Joint|\n",
      "| Discover It PA 1762|     Teazzi Tea Shop|   1.23|2023-05-31|         Expense|       Dining|       Drinks|      -1.23|                  |        false| false|      Credit| Ritu|\n",
      "| Discover It PA 1762|     Teazzi Tea Shop|  24.92|2023-05-31|         Expense|       Dining|       Drinks|     -24.92|                  |        false| false|      Credit| Ritu|\n",
      "| Discover It PA 1762|Onamenu.com Xxxxx...|  76.12|2023-05-31|         Expense|       Dining|         Food|     -76.12|                  |        false| false|      Credit| Ritu|\n",
      "|Checking BoA PA 7462|               Zelle|  100.0|2023-05-31|          Income|     Business|   Tung Duong|      100.0|                  |        false| false|       Debit|   PA|\n",
      "+--------------------+--------------------+-------+----------+----------------+-------------+-------------+-----------+------------------+-------------+------+------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#read master ledger file, this file will also be the output of this notebook\n",
    "#read using pandas then convert to spark dataframe\n",
    "df = spark.createDataFrame(pd.read_excel('../data/other_input/Master Ledger.xlsx',sheet_name=\"Master Ledger\"))\n",
    "\n",
    "#change column type to the appropriate type\n",
    "df = df.withColumn(\"ID\",col(\"ID\").cast(IntegerType()))\\\n",
    "        .withColumn(\"Amount\",col(\"Amount\").cast(DoubleType()))\\\n",
    "        .withColumn(\"Subscriptions\",col(\"Subscriptions\").cast(BooleanType()))\\\n",
    "        .withColumn(\"Return\",col(\"Return\").cast(BooleanType()))\\\n",
    "        .withColumn(\"Real Amount\",col(\"Real Amount\").cast(DoubleType()))\n",
    "#change format of Date\n",
    "df = df.withColumn(\"Date\",to_date(col(\"Date\"),\"MM/dd/yyyy\"))\n",
    "#print Schema\n",
    "df.printSchema()\n",
    "#drop all rows that don't have any ID, fill NaN with blank\n",
    "df = df.dropna(how=\"all\",subset= [\"ID\"]).drop('ID','Limit')\n",
    "df = df.replace('NaN',\"\")\n",
    "#show dataframe\n",
    "df.orderBy(\"ID\", ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read all supplementary inputs\n",
    "acc_meta = spark.read.options(inferSchema='True',header='True').csv('../data/other_input/account_metadata.csv')\n",
    "\n",
    "#create a category mapping based on past data to auto-assign category\n",
    "category_map = df.dropDuplicates(['Account','Item','Categories','Categories 2','Transaction Type']).select(['Account','Item','Categories','Categories 2','Transaction Type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Account: string (nullable = true)\n",
      " |-- Date: date (nullable = true)\n",
      " |-- Amount: double (nullable = true)\n",
      " |-- Account Type: string (nullable = true)\n",
      " |-- Owner: void (nullable = true)\n",
      " |-- Item: string (nullable = true)\n",
      " |-- Real Amount: double (nullable = true)\n",
      " |-- Transaction Type: string (nullable = false)\n",
      " |-- Subscriptions: boolean (nullable = false)\n",
      " |-- Return: boolean (nullable = false)\n",
      "\n",
      "+--------------------+----------+-------+------------+-----+--------------------+-----------+----------------+-------------+------+\n",
      "|             Account|      Date| Amount|Account Type|Owner|                Item|Real Amount|Transaction Type|Subscriptions|Return|\n",
      "+--------------------+----------+-------+------------+-----+--------------------+-----------+----------------+-------------+------+\n",
      "|Checking BoA Ritu...|2023-06-02|  12.32|       Debit| null|              Klarna|     -12.32|         Expense|        false| false|\n",
      "|Checking BoA PA 7462|2023-06-02|  250.0|       Debit| null|Emd Millipore Co ...|      250.0|          Income|        false| false|\n",
      "|Adv Plus Banking ...|2023-06-02|  150.0|       Debit| null|Checkcard 0601 Ku...|     -150.0|         Expense|        false| false|\n",
      "|Adv Plus Banking ...|2023-06-02|    5.0|       Debit| null|          Craigslist|       -5.0|         Expense|        false| false|\n",
      "|Adv Plus Banking ...|2023-06-02|2389.91|       Debit| null|Emd Millipore Co ...|    2389.91|          Income|        false| false|\n",
      "|BoA Customized Ca...|2023-06-01|  57.59|      Credit| null|          Aliexpress|     -57.59|         Expense|        false| false|\n",
      "|Adv Plus Banking ...|2023-06-01| 1600.0|       Debit| null|Bill Pay Check 50...|    -1600.0|         Expense|        false| false|\n",
      "|Adv Plus Banking ...|2023-06-01|  100.0|       Debit| null|               Zelle|     -100.0|         Expense|        false| false|\n",
      "| Chime Checking 4288|2023-06-01|  250.0|       Debit| null|Emd Millipore Co,...|      250.0|          Income|        false| false|\n",
      "|Empower Customer ...|2023-06-01|  33.07|       Debit| null|Interest Paid By ...|      33.07|          Income|        false| false|\n",
      "| Discover It PA 1762|2023-05-31|   1.23|      Credit| null|     Teazzi Tea Shop|      -1.23|         Expense|        false| false|\n",
      "| Discover It PA 1762|2023-05-31|  24.92|      Credit| null|     Teazzi Tea Shop|     -24.92|         Expense|        false| false|\n",
      "| Discover It PA 1762|2023-05-31|  76.12|      Credit| null|Onamenu.com Xxxxx...|     -76.12|         Expense|        false| false|\n",
      "|Checking BoA PA 7462|2023-05-31|  100.0|       Debit| null|               Zelle|      100.0|          Income|        false| false|\n",
      "|Checking BoA Ritu...|2023-05-30|   9.62|       Debit| null|       Market Basket|      -9.62|         Expense|        false| false|\n",
      "|Checking BoA Ritu...|2023-05-30|   8.41|       Debit| null|              Klarna|      -8.41|         Expense|        false| false|\n",
      "|Checking BoA Ritu...|2023-05-30|   9.95|       Debit| null|              Klarna|      -9.95|         Expense|        false| false|\n",
      "|Checking BoA Ritu...|2023-05-30|  194.0|       Debit| null|Pmnt Sent 0526 Re...|     -194.0|         Expense|        false| false|\n",
      "|Adv Plus Banking ...|2023-05-30|   0.05|       Debit| null|Checkcard 0527 Ag...|      -0.05|         Expense|        false| false|\n",
      "|Adv Plus Banking ...|2023-05-30|  200.0|       Debit| null|Fidelity Investments|     -200.0|         Expense|        false| false|\n",
      "+--------------------+----------+-------+------------+-----+--------------------+-----------+----------------+-------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#read all csv files exported from Empower, merge into one spark dataframe\n",
    "path = glob.glob('../data/empower_input/*.csv')\n",
    "df2 = spark.read.options(inferSchema='True',header='True').csv(path)\n",
    "\n",
    "\n",
    "#add more columns to df2 (all empower transactions), so that it matches columns in df (master ledger)\n",
    "df2 = df2.join(acc_meta,on='Account')\\\n",
    ".drop(\"Limit\")\\\n",
    ".filter(~(col('Account Type') == \"Investment\"))\\\n",
    ".withColumn(\"Item\",col(\"Description\")).drop(\"Description\")\\\n",
    ".withColumn(\"Real Amount\",col(\"Amount\"))\\\n",
    ".withColumn(\"Amount\",abs(col(\"Amount\")))\\\n",
    ".withColumn(\"Transaction Type\",when(col(\"Real Amount\") <0, \"Expense\").otherwise(\"Income\"))\\\n",
    ".drop(\"Category\")\\\n",
    ".withColumn(\"Owner\",lit(None))\\\n",
    ".withColumn(\"Subscriptions\",lit(False))\\\n",
    ".withColumn(\"Return\",lit(False))\\\n",
    ".drop(\"Tags\")\\\n",
    "\n",
    "#print schema and show\n",
    "df2.printSchema()\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------+----------+----------------+-------------+-------------+-----------+------------------+-------------+------+------------+-----+-----+\n",
      "|             Account|                Item| Amount|      Date|Transaction Type|   Categories| Categories 2|Real Amount|              Note|Subscriptions|Return|Account Type|Owner|Limit|\n",
      "+--------------------+--------------------+-------+----------+----------------+-------------+-------------+-----------+------------------+-------------+------+------------+-----+-----+\n",
      "|Capital One Quick...|             Comcast|  39.99|2023-06-03|         Expense|      Housing|    Utilities|     -39.99|                  |        false| false|      Credit|   PA| 9000|\n",
      "|Adv Plus Banking ...|Checkcard 0601 Ku...|  150.0|2023-06-02|         Expense|        Legal|  Immigration|     -150.0|                  |        false| false|       Debit|Joint|    0|\n",
      "|Adv Plus Banking ...|          Craigslist|    5.0|2023-06-02|         Expense|        Other|        Event|       -5.0|    Craiglist post|        false| false|       Debit|Joint|    0|\n",
      "|Adv Plus Banking ...|Emd Millipore Co ...|2389.91|2023-06-02|          Income|       Income|          Job|    2389.91|                  |        false| false|       Debit|Joint|    0|\n",
      "|Checking BoA PA 7462|Emd Millipore Co ...|  250.0|2023-06-02|          Income|       Income|          Job|      250.0|                  |        false| false|       Debit|   PA|    0|\n",
      "|Checking BoA Ritu...|              Klarna|  12.32|2023-06-02|         Expense|     Shopping|      Clothes|     -12.32|                  |        false| false|       Debit| Ritu|    0|\n",
      "| Chime Checking 4288|                 Ati|   10.0|2023-06-02|         Expense|Entertainment|       Movies|      -10.0|                  |        false| false|       Debit| Ritu|    0|\n",
      "| Discover It PA 1762|         Chick-fil-a|   16.3|2023-06-02|         Expense|       Dining|         Food|      -16.3|                  |        false| false|      Credit| Ritu| 3550|\n",
      "| Discover It PA 1762|         Chick-fil-a|   2.77|2023-06-02|         Expense|       Dining|       Drinks|      -2.77|                  |        false| false|      Credit| Ritu| 3550|\n",
      "|Adv Plus Banking ...|Bill Pay Check 50...| 1600.0|2023-06-01|         Expense|      Housing|         Rent|    -1600.0|                  |        false| false|       Debit|Joint|    0|\n",
      "|Adv Plus Banking ...|               Zelle|  100.0|2023-06-01|         Expense|     Business|   Tung Duong|     -100.0|                  |        false| false|       Debit|Joint|    0|\n",
      "|BoA Customized Ca...|          Aliexpress|  57.59|2023-06-01|         Expense|        Other|        Event|     -57.59|Wedding Decoration|        false| false|      Credit|   PA| 8000|\n",
      "| Chime Checking 4288|Emd Millipore Co,...|  250.0|2023-06-01|          Income|       Income|          Job|      250.0|                  |        false| false|       Debit| Ritu|    0|\n",
      "|Empower Customer ...|Interest Paid By ...|  33.07|2023-06-01|          Income|       Income|     Interest|      33.07|                  |        false| false|       Debit|   PA|    0|\n",
      "|Checking BoA PA 7462|               Zelle|  100.0|2023-05-31|          Income|     Business|   Tung Duong|      100.0|                  |        false| false|       Debit|   PA|    0|\n",
      "| Discover It PA 1762|Onamenu.com Xxxxx...|  76.12|2023-05-31|         Expense|       Dining|         Food|     -76.12|                  |        false| false|      Credit| Ritu| 3550|\n",
      "| Discover It PA 1762|     Teazzi Tea Shop|  24.92|2023-05-31|         Expense|       Dining|       Drinks|     -24.92|                  |        false| false|      Credit| Ritu| 3550|\n",
      "| Discover It PA 1762|     Teazzi Tea Shop|   1.23|2023-05-31|         Expense|       Dining|       Drinks|      -1.23|                  |        false| false|      Credit| Ritu| 3550|\n",
      "| Robinhoodinvestment|Penn $26 Call 6/2...|    0.0|2023-05-31|         Income3|   Investment|Transactional|        0.0|                  |        false| false|  Investment|Joint|    0|\n",
      "| Robinhoodinvestment|Penn $26 Call 6/2...|   10.0|2023-05-31|         Income3|   Investment|Transactional|       10.0|                  |        false| false|  Investment|Joint|    0|\n",
      "+--------------------+--------------------+-------+----------+----------------+-------------+-------------+-----------+------------------+-------------+------+------------+-----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#find the latest date in master ledger file\n",
    "max_date = df.select(max(\"Date\")).first()[0]\n",
    "\n",
    "# union master ledger with empower, where the empower dataframe is filtered on max_date - 5. \n",
    "# This is to ensure it captures all transactions, because sometimes the transactions are updated few days after, so 5 days is a good limit. \n",
    "df3 = df.unionByName(df2.filter(col(\"Date\")> lit(max_date)-5), allowMissingColumns=True)\n",
    "df3 = df3.drop(\"Account Type\",\"Owner\").join(acc_meta, on = 'Account').na.fill(\"\")\n",
    "\n",
    "#auto-assign category using category mapping \n",
    "df4 = df3.filter(col(\"Categories\") == lit(\"\")).drop('Categories','Categories 2').join(category_map, on=['Account','Item','Transaction Type'])\n",
    "df3 = df3.filter(~(col(\"Categories\") == lit(\"\"))).unionByName(df4, allowMissingColumns=True)\n",
    "\n",
    "#further drop duplicates, in case the Note column are already filled using window partition\n",
    "#group all transactions which have the same date, account, item and amount into a partition, then assign row number\n",
    "#if any of them has row_number value higher than 1, and their Note columns is blank, indicate these as dup and filter them out\n",
    "window = Window.partitionBy(['Date','Account','Item','Real Amount']).orderBy(col(\"Note\").desc())\n",
    "df3 = df3.withColumn(\"row_number\",row_number().over(window))\\\n",
    "    .withColumn('dup',when((col('row_number')>1) & (col('Note') == \"\"),True).otherwise(False))\\\n",
    "    .filter(col('dup') == False)\\\n",
    "    .drop(\"row_number\",\"dup\")\\\n",
    "    .orderBy(\"Date\")\n",
    "\n",
    "\n",
    "#resort dataframe by descending date\n",
    "df3.orderBy(\"Date\", ascending=False).show(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export to csv and add column ID\n",
    "df3.toPandas().to_csv(\"../data/output/out.csv\", index_label=\"ID\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
