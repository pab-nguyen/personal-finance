{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import StringType,BooleanType,DateType,IntegerType,DoubleType\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "#create spark Session\n",
    "spark = SparkSession.builder.appName(\"PF\").config(\"spark.sql.caseSensitive\", \"True\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: integer (nullable = true)\n",
      " |-- Account: string (nullable = true)\n",
      " |-- Item: string (nullable = true)\n",
      " |-- Amount: double (nullable = true)\n",
      " |-- Date: date (nullable = true)\n",
      " |-- Transaction Type: string (nullable = true)\n",
      " |-- Categories: string (nullable = true)\n",
      " |-- Categories 2: string (nullable = true)\n",
      " |-- Real Amount: double (nullable = true)\n",
      " |-- Note: string (nullable = true)\n",
      " |-- Subscriptions: boolean (nullable = true)\n",
      " |-- Return: boolean (nullable = true)\n",
      " |-- Limit: double (nullable = true)\n",
      " |-- Account Type: string (nullable = true)\n",
      " |-- Owner: string (nullable = true)\n",
      "\n",
      "+----+--------------------+--------------------+------+----------+----------------+-------------+--------------+-----------+----------------+-------------+------+------+------------+-----+\n",
      "|  ID|             Account|                Item|Amount|      Date|Transaction Type|   Categories|  Categories 2|Real Amount|            Note|Subscriptions|Return| Limit|Account Type|Owner|\n",
      "+----+--------------------+--------------------+------+----------+----------------+-------------+--------------+-----------+----------------+-------------+------+------+------------+-----+\n",
      "|2824| Robinhoodinvestment|Penn $26 Call 6/2...|  10.0|2023-05-31|         Income3|   Investment| Transactional|       10.0|                |        false| false|   NaN|  Investment|Joint|\n",
      "|2823| Robinhoodinvestment|Penn $26 Call 6/2...|   0.0|2023-05-31|         Income3|   Investment| Transactional|        0.0|                |        false| false|   NaN|  Investment|Joint|\n",
      "|2822|Roth Ira - Ending...|Cash Contribution...| 200.0|2023-05-30|         Income3|   Investment|       Deposit|      200.0|                |        false| false|   NaN|  Investment|   PA|\n",
      "|2821|Checking BoA Ritu...|Pmnt Sent 0526 Re...| 194.0|2023-05-30|        Expense2|Transactional| Transactional|     -194.0|                |        false| false|   NaN|       Debit| Ritu|\n",
      "|2820|Checking BoA Ritu...|       Market Basket|  9.62|2023-05-30|         Expense|       People|         Gifts|      -9.62|terrence's plant|        false| false|   NaN|       Debit| Ritu|\n",
      "|2819|Checking BoA Ritu...|              Klarna|  8.41|2023-05-30|         Expense|     Shopping|       Clothes|      -8.41|                |        false| false|   NaN|       Debit| Ritu|\n",
      "|2818|Checking BoA Ritu...|              Klarna|  9.95|2023-05-30|         Expense|     Shopping|       Clothes|      -9.95|                |        false| false|   NaN|       Debit| Ritu|\n",
      "|2817|Adv Plus Banking ...|      The Home Depot| 36.25|2023-05-30|         Expense|     Shopping|     Household|     -36.25|                |        false| false|   NaN|       Debit|Joint|\n",
      "|2816|Adv Plus Banking ...|              Klarna|397.64|2023-05-30|         Expense|     Shopping|        Luxury|    -397.64|   birthday gift|        false| false|   NaN|       Debit|Joint|\n",
      "|2815|Adv Plus Banking ...|Fidelity Investments| 200.0|2023-05-30|        Expense3|   Investment|       Deposit|     -200.0|                |        false| false|   NaN|       Debit|Joint|\n",
      "|2814|Adv Plus Banking ...|        Cvs Pharmacy| 13.76|2023-05-30|         Expense|     Shopping|      Personal|     -13.76|                |        false| false|   NaN|       Debit|Joint|\n",
      "|2813|Adv Plus Banking ...|Checkcard 0527 Ag...|  0.05|2023-05-30|         Expense|       Travel|Transportation|      -0.05|                |        false| false|   NaN|       Debit|Joint|\n",
      "|2812|Adv Plus Banking ...|Checkcard 0527 Ag...|  1.66|2023-05-30|         Expense|       Travel|Transportation|      -1.66|                |        false| false|   NaN|       Debit|Joint|\n",
      "|2811|Adv Plus Banking ...|Checkcard 0526 Qa...| 313.0|2023-05-30|        Expense2|Transactional| Transactional|     -313.0|                |        false| false|   NaN|       Debit|Joint|\n",
      "|2810| Discover It PA 1762|Xing Fu Tang Mont...| 14.39|2023-05-28|         Expense|       Travel|          Food|     -14.39|                |        false| false|3550.0|      Credit| Ritu|\n",
      "|2809| Discover It PA 1762|           Six Flags| 18.35|2023-05-28|         Expense|       Travel|Transportation|     -18.35|                |        false| false|3550.0|      Credit| Ritu|\n",
      "|2808| Discover It PA 1762|Popeyes Louisiana...| 23.84|2023-05-28|         Expense|       Travel|          Food|     -23.84|                |        false| false|3550.0|      Credit| Ritu|\n",
      "|2807|Costco Anywhere b...|Allo Mon Coco Sai...| 84.76|2023-05-28|         Expense|       Travel|          Food|     -84.76|                |        false| false|3000.0|      Credit|   PA|\n",
      "|2806| Discover It PA 1762|Yifang Fruit The ...| 32.65|2023-05-27|         Expense|       Travel|          Food|     -32.65|                |        false| false|3550.0|      Credit| Ritu|\n",
      "|2805|Costco Anywhere b...|Restaurant La Ban...| 78.54|2023-05-27|         Expense|       Travel|          Food|     -78.54|                |        false| false|3000.0|      Credit|   PA|\n",
      "+----+--------------------+--------------------+------+----------+----------------+-------------+--------------+-----------+----------------+-------------+------+------+------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#read all supplementary inputs\n",
    "acc_meta = spark.read.options(inferSchema='True',header='True').csv('../data/other_input/account_metadata.csv')\n",
    "\n",
    "\n",
    "#read master ledger file, this file will also be the output of this notebook\n",
    "#read using pandas then convert to spark dataframe\n",
    "df = spark.createDataFrame(pd.read_excel('../data/other_input/Master Ledger.xlsx',sheet_name=\"Master Ledger\"))\n",
    "\n",
    "#change column type to the appropriate type\n",
    "df = df.withColumn(\"ID\",col(\"ID\").cast(IntegerType()))\\\n",
    "        .withColumn(\"Amount\",col(\"Amount\").cast(DoubleType()))\\\n",
    "        .withColumn(\"Subscriptions\",col(\"Subscriptions\").cast(BooleanType()))\\\n",
    "        .withColumn(\"Return\",col(\"Return\").cast(BooleanType()))\\\n",
    "        .withColumn(\"Real Amount\",col(\"Real Amount\").cast(DoubleType()))\n",
    "#change format of Date\n",
    "df = df.withColumn(\"Date\",to_date(col(\"Date\"),\"MM/dd/yyyy\"))\n",
    "#print Schema\n",
    "df.printSchema()\n",
    "#drop all rows that don't have any ID, fill NaN with blank\n",
    "df = df.dropna(how=\"all\",subset= [\"ID\"])\n",
    "df = df.replace('NaN',\"\")\n",
    "#show dataframe\n",
    "df.orderBy(\"ID\", ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: date (nullable = true)\n",
      " |-- Account: string (nullable = true)\n",
      " |-- Amount: double (nullable = true)\n",
      " |-- ID: integer (nullable = false)\n",
      " |-- Item: string (nullable = true)\n",
      " |-- Real Amount: double (nullable = true)\n",
      " |-- Account Type: void (nullable = true)\n",
      " |-- Owner: void (nullable = true)\n",
      " |-- Transaction Type: string (nullable = false)\n",
      " |-- Subscriptions: boolean (nullable = false)\n",
      " |-- Return: boolean (nullable = false)\n",
      "\n",
      "+----------+--------------------+------+---+--------------------+-----------+------------+-----+----------------+-------------+------+\n",
      "|      Date|             Account|Amount| ID|                Item|Real Amount|Account Type|Owner|Transaction Type|Subscriptions|Return|\n",
      "+----------+--------------------+------+---+--------------------+-----------+------------+-----+----------------+-------------+------+\n",
      "|2023-05-31|401(k) Savings An...|   5.0|  1|Fid Us Bond Idx -...|        5.0|        null| null|          Income|        false| false|\n",
      "|2023-05-31|Health Savings Ac...|  3.67|  1|Dividend Received...|       3.67|        null| null|          Income|        false| false|\n",
      "|2023-05-31|Health Savings Ac...|  3.67|  1|Reinvestment Fide...|      -3.67|        null| null|         Expense|        false| false|\n",
      "|2023-05-31|Roth Ira - Ending...|  0.08|  1|Dividend Received...|       0.08|        null| null|          Income|        false| false|\n",
      "|2023-05-31|Roth Ira - Ending...|  0.22|  1|Dividend Received...|       0.22|        null| null|          Income|        false| false|\n",
      "|2023-05-31| Robinhoodinvestment|  0.02|  1|            Interest|       0.02|        null| null|          Income|        false| false|\n",
      "|2023-05-31|Checking BoA PA 7462| 100.0|  1|               Zelle|      100.0|        null| null|          Income|        false| false|\n",
      "|2023-05-31| Robinhoodinvestment|  10.0|  1|Penn $26 Call 6/2...|       10.0|        null| null|          Income|        false| false|\n",
      "|2023-05-31| Robinhoodinvestment|   0.0|  1|Penn $26 Call 6/2...|        0.0|        null| null|          Income|        false| false|\n",
      "|2023-05-30|Roth Ira - Ending...| 200.0|  1|Cash Contribution...|      200.0|        null| null|          Income|        false| false|\n",
      "|2023-05-30|Checking BoA Ritu...|  9.62|  1|       Market Basket|      -9.62|        null| null|         Expense|        false| false|\n",
      "|2023-05-30|Checking BoA Ritu...|  8.41|  1|              Klarna|      -8.41|        null| null|         Expense|        false| false|\n",
      "|2023-05-30|Checking BoA Ritu...|  9.95|  1|              Klarna|      -9.95|        null| null|         Expense|        false| false|\n",
      "|2023-05-30|Checking BoA Ritu...| 194.0|  1|Pmnt Sent 0526 Re...|     -194.0|        null| null|         Expense|        false| false|\n",
      "|2023-05-30|Adv Plus Banking ...|  0.05|  1|Checkcard 0527 Ag...|      -0.05|        null| null|         Expense|        false| false|\n",
      "|2023-05-30|Adv Plus Banking ...| 200.0|  1|Fidelity Investments|     -200.0|        null| null|         Expense|        false| false|\n",
      "|2023-05-30|Adv Plus Banking ...| 13.76|  1|        Cvs Pharmacy|     -13.76|        null| null|         Expense|        false| false|\n",
      "|2023-05-30|Adv Plus Banking ...| 36.25|  1|      The Home Depot|     -36.25|        null| null|         Expense|        false| false|\n",
      "|2023-05-30|Adv Plus Banking ...|397.64|  1|              Klarna|    -397.64|        null| null|         Expense|        false| false|\n",
      "|2023-05-30|Adv Plus Banking ...|  1.66|  1|Checkcard 0527 Ag...|      -1.66|        null| null|         Expense|        false| false|\n",
      "+----------+--------------------+------+---+--------------------+-----------+------------+-----+----------------+-------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#read all csv files exported from Empower, merge into one spark dataframe\n",
    "path = glob.glob('../data/empower_input/*.csv')\n",
    "df2 = spark.read.options(inferSchema='True',header='True').csv(path)\n",
    "\n",
    "\n",
    "#add more columns to df2 (all empower transactions), so that it matches columns in df (master ledger)\n",
    "df2 = df2.withColumn(\"ID\",lit(1))\\\n",
    ".withColumn(\"Item\",col(\"Description\")).drop(\"Description\")\\\n",
    ".withColumn(\"Real Amount\",col(\"Amount\"))\\\n",
    ".withColumn(\"Amount\",abs(col(\"Amount\")))\\\n",
    ".withColumn(\"Account Type\",lit(None))\\\n",
    ".withColumn(\"Owner\",lit(None))\\\n",
    ".withColumn(\"Transaction Type\",when(col(\"Real Amount\") <0, \"Expense\").otherwise(\"Income\"))\\\n",
    ".drop(\"Category\")\\\n",
    ".withColumn(\"Owner\",lit(None))\\\n",
    ".withColumn(\"Subscriptions\",lit(False))\\\n",
    ".withColumn(\"Return\",lit(False))\\\n",
    ".drop(\"Tags\")\n",
    "\n",
    "#print schema and show\n",
    "df2.printSchema()\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------+----------+----------------+-------------+--------------+-----------+----------------+-------------+------+-----+------------+-----+-----+\n",
      "|             Account|                Item|Amount|      Date|Transaction Type|   Categories|  Categories 2|Real Amount|            Note|Subscriptions|Return|Limit|Account Type|Owner|Limit|\n",
      "+--------------------+--------------------+------+----------+----------------+-------------+--------------+-----------+----------------+-------------+------+-----+------------+-----+-----+\n",
      "|401(k) Savings An...|Fid Us Bond Idx -...|   5.0|2023-05-31|          Income|             |              |        5.0|                |        false| false| null|  Investment|   PA|    0|\n",
      "|Checking BoA PA 7462|               Zelle| 100.0|2023-05-31|          Income|             |              |      100.0|                |        false| false| null|       Debit|   PA|    0|\n",
      "|Health Savings Ac...|Dividend Received...|  3.67|2023-05-31|          Income|             |              |       3.67|                |        false| false| null|  Investment|   PA|    0|\n",
      "|Health Savings Ac...|Reinvestment Fide...|  3.67|2023-05-31|         Expense|             |              |      -3.67|                |        false| false| null|  Investment|   PA|    0|\n",
      "| Robinhoodinvestment|            Interest|  0.02|2023-05-31|          Income|             |              |       0.02|                |        false| false| null|  Investment|Joint|    0|\n",
      "| Robinhoodinvestment|Penn $26 Call 6/2...|   0.0|2023-05-31|         Income3|   Investment| Transactional|        0.0|                |        false| false|  NaN|  Investment|Joint|    0|\n",
      "| Robinhoodinvestment|Penn $26 Call 6/2...|  10.0|2023-05-31|         Income3|   Investment| Transactional|       10.0|                |        false| false|  NaN|  Investment|Joint|    0|\n",
      "|Roth Ira - Ending...|Dividend Received...|  0.08|2023-05-31|          Income|             |              |       0.08|                |        false| false| null|  Investment|   PA|    0|\n",
      "|Roth Ira - Ending...|Dividend Received...|  0.22|2023-05-31|          Income|             |              |       0.22|                |        false| false| null|  Investment|   PA|    0|\n",
      "|Adv Plus Banking ...|Checkcard 0527 Ag...|  0.05|2023-05-30|         Expense|       Travel|Transportation|      -0.05|                |        false| false|  NaN|       Debit|Joint|    0|\n",
      "|Adv Plus Banking ...|Checkcard 0526 Qa...| 313.0|2023-05-30|        Expense2|Transactional| Transactional|     -313.0|                |        false| false|  NaN|       Debit|Joint|    0|\n",
      "|Adv Plus Banking ...|Checkcard 0527 Ag...|  1.66|2023-05-30|         Expense|       Travel|Transportation|      -1.66|                |        false| false|  NaN|       Debit|Joint|    0|\n",
      "|Adv Plus Banking ...|        Cvs Pharmacy| 13.76|2023-05-30|         Expense|     Shopping|      Personal|     -13.76|                |        false| false|  NaN|       Debit|Joint|    0|\n",
      "|Adv Plus Banking ...|Fidelity Investments| 200.0|2023-05-30|        Expense3|   Investment|       Deposit|     -200.0|                |        false| false|  NaN|       Debit|Joint|    0|\n",
      "|Adv Plus Banking ...|              Klarna|397.64|2023-05-30|         Expense|     Shopping|        Luxury|    -397.64|   birthday gift|        false| false|  NaN|       Debit|Joint|    0|\n",
      "|Adv Plus Banking ...|      The Home Depot| 36.25|2023-05-30|         Expense|     Shopping|     Household|     -36.25|                |        false| false|  NaN|       Debit|Joint|    0|\n",
      "|Checking BoA Ritu...|              Klarna|  9.95|2023-05-30|         Expense|     Shopping|       Clothes|      -9.95|                |        false| false|  NaN|       Debit| Ritu|    0|\n",
      "|Checking BoA Ritu...|              Klarna|  8.41|2023-05-30|         Expense|     Shopping|       Clothes|      -8.41|                |        false| false|  NaN|       Debit| Ritu|    0|\n",
      "|Checking BoA Ritu...|       Market Basket|  9.62|2023-05-30|         Expense|       People|         Gifts|      -9.62|terrence's plant|        false| false|  NaN|       Debit| Ritu|    0|\n",
      "|Checking BoA Ritu...|Pmnt Sent 0526 Re...| 194.0|2023-05-30|        Expense2|Transactional| Transactional|     -194.0|                |        false| false|  NaN|       Debit| Ritu|    0|\n",
      "+--------------------+--------------------+------+----------+----------------+-------------+--------------+-----------+----------------+-------------+------+-----+------------+-----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#find the latest date in master ledger file\n",
    "max_date = df.select(max(\"Date\")).first()[0]\n",
    "\n",
    "# union master ledger with empower, where the empower dataframe is filtered on max_date - 5. \n",
    "# This is to ensure it captures all transactions, because sometimes the transactions are updated few days after, so 5 days is a good limit. \n",
    "df3 = df.unionByName(df2.filter(col(\"Date\")> lit(max_date)-5), allowMissingColumns=True)\n",
    "df3 = df3.drop(\"ID\",\"Account Type\",\"Owner\").join(acc_meta, on = 'Account').na.fill(\"\")\n",
    "\n",
    "#further drop duplicates, in case the Note column are already filled using window partition\n",
    "#group all transactions which have the same date, account, item and amount into a partition, then assign row number\n",
    "#if any of them has row_number value higher than 1, and their Note columns is blank, indicate these as dup and filter them out\n",
    "window = Window.partitionBy(['Date','Account','Item','Real Amount']).orderBy(col(\"Note\").desc())\n",
    "df3 = df3.withColumn(\"row_number\",row_number().over(window))\\\n",
    "    .withColumn('dup',when((col('row_number')>1) & (col('Note') == \"\"),True).otherwise(False))\\\n",
    "    .filter(col('dup') == False)\\\n",
    "    .drop(\"row_number\",\"dup\")\\\n",
    "    .orderBy(\"Date\")\n",
    "\n",
    "#resort dataframe by descending date\n",
    "df3.orderBy(\"Date\", ascending=False).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export to csv and add column ID\n",
    "df3.toPandas().to_csv(\"../data/output/out.csv\", index_label=\"ID\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
