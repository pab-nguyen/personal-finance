{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import StringType,BooleanType,DateType,IntegerType,DoubleType\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "#create spark Session\n",
    "spark = SparkSession.builder.appName(\"PF\").config(\"spark.sql.caseSensitive\", \"True\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: integer (nullable = true)\n",
      " |-- Account: string (nullable = true)\n",
      " |-- Item: string (nullable = true)\n",
      " |-- Amount: double (nullable = true)\n",
      " |-- Date: date (nullable = true)\n",
      " |-- Transaction Type: string (nullable = true)\n",
      " |-- Categories: string (nullable = true)\n",
      " |-- Categories 2: string (nullable = true)\n",
      " |-- Real Amount: double (nullable = true)\n",
      " |-- Note: string (nullable = true)\n",
      " |-- Subscriptions: boolean (nullable = true)\n",
      " |-- Return: boolean (nullable = true)\n",
      " |-- Account Type: string (nullable = true)\n",
      " |-- Owner: string (nullable = true)\n",
      "\n",
      "+----+--------------------+--------------------+------+----------+----------------+-------------+--------------------+-----------+------------------+-------------+------+------------+-----+\n",
      "|  ID|             Account|                Item|Amount|      Date|Transaction Type|   Categories|        Categories 2|Real Amount|              Note|Subscriptions|Return|Account Type|Owner|\n",
      "+----+--------------------+--------------------+------+----------+----------------+-------------+--------------------+-----------+------------------+-------------+------+------------+-----+\n",
      "|2785|Checking BoA Ritu...|              Klarna|  3.83|2023-05-24|         Expense|     Shopping|             Clothes|      -3.83|                  |        false| false|       Debit| Ritu|\n",
      "|2784|Checking BoA Ritu...|              Klarna| 18.79|2023-05-24|         Expense|     Shopping|             Clothes|     -18.79|                  |        false| false|       Debit| Ritu|\n",
      "|2783|Checking BoA Ritu...|      Dunkin' Donuts|   5.0|2023-05-24|         Expense|       Dining|              Drinks|       -5.0|                  |        false| false|       Debit| Ritu|\n",
      "|2782|Checking BoA Ritu...|          Aliexpress| 158.5|2023-05-24|         Expense|        Other|               Event|     -158.5|Wedding Decoration|        false| false|       Debit| Ritu|\n",
      "|2781|Adv Plus Banking ...|             Tj Maxx| 45.35|2023-05-24|         Expense|     Shopping|             Clothes|     -45.35|                  |        false| false|       Debit|Joint|\n",
      "|2780|Adv Plus Banking ...|              Target| 17.13|2023-05-24|         Expense|        Other|               Event|     -17.13|Wedding Decoration|        false| false|       Debit|Joint|\n",
      "|2779|Adv Plus Banking ...|              Target|  17.5|2023-05-24|         Expense|        Other|               Event|      -17.5|Wedding Decoration|        false| false|       Debit|Joint|\n",
      "|2778|Adv Plus Banking ...|         Dollar Tree| 17.27|2023-05-24|         Expense|        Other|               Event|     -17.27|Wedding Decoration|        false| false|       Debit|Joint|\n",
      "|2777|Adv Plus Banking ...|     Bank Of America| 800.0|2023-05-24|         Income2|Transactional|       Transactional|      800.0|                  |        false| false|       Debit|Joint|\n",
      "|2776|Adv Plus Banking ...|American Wagering...|  50.0|2023-05-24|         Expense|Entertainment|             Betting|      -50.0|                  |        false| false|       Debit|Joint|\n",
      "|2775|Checking BoA Ritu...|            Michaels|  5.94|2023-05-23|         Expense|        Other|               Event|      -5.94|Wedding Decoration|        false| false|       Debit| Ritu|\n",
      "|2774|Checking BoA Ritu...|              Klarna|  8.13|2023-05-23|         Expense|     Shopping|             Clothes|      -8.13|                  |        false| false|       Debit| Ritu|\n",
      "|2773|Checking BoA PA 7462|               Zelle|   5.0|2023-05-23|          Income|       Income|Facebook Martketp...|        5.0|                  |        false| false|       Debit|   PA|\n",
      "|2772|Amazon Prime Cred...|              Amazon| 11.76|2023-05-23|         Expense|        Other|               Event|     -11.76|Wedding Decoration|        false| false|      Credit| Ritu|\n",
      "|2771|Adv Plus Banking ...|             Tj Maxx|  21.3|2023-05-23|         Expense|     Shopping|             Clothes|      -21.3|                  |        false| false|       Debit|Joint|\n",
      "|2770|Adv Plus Banking ...|              Savers|  6.36|2023-05-23|         Expense|        Other|               Event|      -6.36|Wedding Decoration|        false| false|       Debit|Joint|\n",
      "|2769|Adv Plus Banking ...|           Marshalls| 41.98|2023-05-23|         Expense|     Shopping|             Clothes|     -41.98|                  |        false| false|       Debit|Joint|\n",
      "|2768|Adv Plus Banking ...|         Dollar Tree| 30.55|2023-05-23|         Expense|        Other|               Event|     -30.55|Wedding Decoration|        false| false|       Debit|Joint|\n",
      "|2767|Adv Plus Banking ...|            Citibank|305.65|2023-05-23|        Expense4|         Bill|         Credit Card|    -305.65|                  |        false| false|       Debit|Joint|\n",
      "|2766| Robinhoodinvestment|Deposit From Adv ...| 100.0|2023-05-22|         Income3|   Investment|             Deposit|      100.0|                  |        false| false|  Investment|Joint|\n",
      "+----+--------------------+--------------------+------+----------+----------------+-------------+--------------------+-----------+------------------+-------------+------+------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#read all supplementary inputs\n",
    "acc_meta = spark.read.options(inferSchema='True',header='True').csv('../data/other_input/account_metadata.csv')\n",
    "\n",
    "\n",
    "#read master ledger file, this file will also be the output of this notebook\n",
    "#read using pandas then convert to spark dataframe\n",
    "df = spark.createDataFrame(pd.read_excel('../data/other_input/Master Ledger.xlsx',sheet_name=\"Master Ledger\"))\n",
    "\n",
    "#change column type to the appropriate type\n",
    "df = df.withColumn(\"ID\",col(\"ID\").cast(IntegerType()))\\\n",
    "        .withColumn(\"Amount\",col(\"Amount\").cast(DoubleType()))\\\n",
    "        .withColumn(\"Subscriptions\",col(\"Subscriptions\").cast(BooleanType()))\\\n",
    "        .withColumn(\"Return\",col(\"Return\").cast(BooleanType()))\\\n",
    "        .withColumn(\"Real Amount\",col(\"Real Amount\").cast(DoubleType()))\n",
    "#change format of Date\n",
    "df = df.withColumn(\"Date\",to_date(col(\"Date\"),\"MM/dd/yyyy\"))\n",
    "#print Schema\n",
    "df.printSchema()\n",
    "#drop all rows that don't have any ID, fill NaN with blank\n",
    "df = df.dropna(how=\"all\",subset= [\"ID\"])\n",
    "df = df.replace('NaN',\"\")\n",
    "#show dataframe\n",
    "df.orderBy(\"ID\", ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: date (nullable = true)\n",
      " |-- Account: string (nullable = true)\n",
      " |-- Amount: double (nullable = true)\n",
      " |-- ID: integer (nullable = false)\n",
      " |-- Item: string (nullable = true)\n",
      " |-- Real Amount: double (nullable = true)\n",
      " |-- Account Type: void (nullable = true)\n",
      " |-- Owner: void (nullable = true)\n",
      " |-- Transaction Type: string (nullable = false)\n",
      " |-- Subscriptions: boolean (nullable = false)\n",
      " |-- Return: boolean (nullable = false)\n",
      "\n",
      "+----------+--------------------+------+---+--------------------+-----------+------------+-----+----------------+-------------+------+\n",
      "|      Date|             Account|Amount| ID|                Item|Real Amount|Account Type|Owner|Transaction Type|Subscriptions|Return|\n",
      "+----------+--------------------+------+---+--------------------+-----------+------------+-----+----------------+-------------+------+\n",
      "|2023-05-24|Checking BoA Ritu...| 158.5|  1|          Aliexpress|     -158.5|        null| null|         Expense|        false| false|\n",
      "|2023-05-24|Checking BoA Ritu...|  3.83|  1|              Klarna|      -3.83|        null| null|         Expense|        false| false|\n",
      "|2023-05-24|Checking BoA Ritu...| 18.79|  1|              Klarna|     -18.79|        null| null|         Expense|        false| false|\n",
      "|2023-05-24|Checking BoA Ritu...|   5.0|  1|      Dunkin' Donuts|       -5.0|        null| null|         Expense|        false| false|\n",
      "|2023-05-24|Adv Plus Banking ...|  50.0|  1|American Wagering...|      -50.0|        null| null|         Expense|        false| false|\n",
      "|2023-05-24|Adv Plus Banking ...| 17.27|  1|         Dollar Tree|     -17.27|        null| null|         Expense|        false| false|\n",
      "|2023-05-24|Adv Plus Banking ...| 45.35|  1|             Tj Maxx|     -45.35|        null| null|         Expense|        false| false|\n",
      "|2023-05-24|Adv Plus Banking ...|  17.5|  1|              Target|      -17.5|        null| null|         Expense|        false| false|\n",
      "|2023-05-24|Adv Plus Banking ...| 17.13|  1|              Target|     -17.13|        null| null|         Expense|        false| false|\n",
      "|2023-05-24|Adv Plus Banking ...| 800.0|  1|     Bank Of America|      800.0|        null| null|          Income|        false| false|\n",
      "|2023-05-23|Checking BoA PA 7462|   5.0|  1|               Zelle|        5.0|        null| null|          Income|        false| false|\n",
      "|2023-05-23|Adv Plus Banking ...|305.65|  1|            Citibank|    -305.65|        null| null|         Expense|        false| false|\n",
      "|2023-05-23|Adv Plus Banking ...|  21.3|  1|             Tj Maxx|      -21.3|        null| null|         Expense|        false| false|\n",
      "|2023-05-23|Adv Plus Banking ...| 30.55|  1|         Dollar Tree|     -30.55|        null| null|         Expense|        false| false|\n",
      "|2023-05-23|Adv Plus Banking ...| 41.98|  1|           Marshalls|     -41.98|        null| null|         Expense|        false| false|\n",
      "|2023-05-23|Adv Plus Banking ...|  6.36|  1|              Savers|      -6.36|        null| null|         Expense|        false| false|\n",
      "|2023-05-23|Checking BoA Ritu...|  8.13|  1|              Klarna|      -8.13|        null| null|         Expense|        false| false|\n",
      "|2023-05-23|Checking BoA Ritu...|  5.94|  1|            Michaels|      -5.94|        null| null|         Expense|        false| false|\n",
      "|2023-05-23|Amazon Prime Cred...| 11.76|  1|              Amazon|     -11.76|        null| null|         Expense|        false| false|\n",
      "|2023-05-22|Amazon Prime Cred...| 96.27|  1|              Amazon|     -96.27|        null| null|         Expense|        false| false|\n",
      "+----------+--------------------+------+---+--------------------+-----------+------------+-----+----------------+-------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#read all csv files exported from Empower, merge into one spark dataframe\n",
    "path = glob.glob('../data/empower_input/*.csv')\n",
    "df2 = spark.read.options(inferSchema='True',header='True').csv(path)\n",
    "\n",
    "\n",
    "#add more columns to df2 (all empower transactions), so that it matches columns in df (master ledger)\n",
    "df2 = df2.withColumn(\"ID\",lit(1))\\\n",
    ".withColumn(\"Item\",col(\"Description\")).drop(\"Description\")\\\n",
    ".withColumn(\"Real Amount\",col(\"Amount\"))\\\n",
    ".withColumn(\"Amount\",abs(col(\"Amount\")))\\\n",
    ".withColumn(\"Account Type\",lit(None))\\\n",
    ".withColumn(\"Owner\",lit(None))\\\n",
    ".withColumn(\"Transaction Type\",when(col(\"Real Amount\") <0, \"Expense\").otherwise(\"Income\"))\\\n",
    ".drop(\"Category\")\\\n",
    ".withColumn(\"Owner\",lit(None))\\\n",
    ".withColumn(\"Subscriptions\",lit(False))\\\n",
    ".withColumn(\"Return\",lit(False))\\\n",
    ".drop(\"Tags\")\n",
    "\n",
    "#print schema and show\n",
    "df2.printSchema()\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# union master ledger with empower, then \u001b[39;00m\n\u001b[0;32m      2\u001b[0m max_date \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mselect(\u001b[39mmax\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mDate\u001b[39m\u001b[39m\"\u001b[39m))\u001b[39m.\u001b[39mfirst()[\u001b[39m0\u001b[39m]\n\u001b[1;32m----> 3\u001b[0m df3 \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39munionByName(df2\u001b[39m.\u001b[39mfilter(col(\u001b[39m\"\u001b[39m\u001b[39mDate\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m>\u001b[39m lit(max_date)\u001b[39m-\u001b[39m\u001b[39m5\u001b[39m), allowMissingColumns\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m      4\u001b[0m df3 \u001b[39m=\u001b[39m df3\u001b[39m.\u001b[39mdrop(\u001b[39m\"\u001b[39m\u001b[39mID\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39mAccount Type\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39mOwner\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mjoin(acc_meta, on \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mAccount\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mna\u001b[39m.\u001b[39mfill(\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m df3 \u001b[39m=\u001b[39m df3\u001b[39m.\u001b[39mdropDuplicates([\u001b[39m'\u001b[39m\u001b[39mDate\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mAccount\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mItem\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mReal Amount\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mNote\u001b[39m\u001b[39m'\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df2' is not defined"
     ]
    }
   ],
   "source": [
    "#find the latest date in master ledger file\n",
    "max_date = df.select(max(\"Date\")).first()[0]\n",
    "\n",
    "# union master ledger with empower, where the empower dataframe is filtered on max_date - 5. \n",
    "# This is to ensure it captures all transactions, because sometimes the transactions are updated few days after, so 5 days is a good limit. \n",
    "df3 = df.unionByName(df2.filter(col(\"Date\")> lit(max_date)-5), allowMissingColumns=True)\n",
    "df3 = df3.drop(\"ID\",\"Account Type\",\"Owner\").join(acc_meta, on = 'Account').na.fill(\"\")\n",
    "\n",
    "#further drop duplicates, in case the Note column are already filled using window partition\n",
    "#group all transactions which have the same date, account, item and amount into a partition, then assign row number\n",
    "#if any of them has row_number value higher than 1, and their Note columns is blank, indicate these as dup and filter them out\n",
    "window = Window.partitionBy(['Date','Account','Item','Real Amount']).orderBy(col(\"Note\").desc())\n",
    "df3 = df3.withColumn(\"row_number\",row_number().over(window))\\\n",
    "    .withColumn('dup',when((col('row_number')>1) & (col('Note') == \"\"),True).otherwise(False))\\\n",
    "    .filter(col('dup') == False)\\\n",
    "    .drop(\"row_number\",\"dup\")\\\n",
    "    .orderBy(\"Date\")\n",
    "\n",
    "#resort dataframe by descending date\n",
    "df3.orderBy(\"Date\", ascending=False).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "#export to csv and add column ID\n",
    "df3.toPandas().to_csv(\"../data/output/out.csv\", index_label=\"ID\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
